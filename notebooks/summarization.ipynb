{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarise Daily Snippets into Journal Entries\n",
    "\n",
    "This notebook explores different models to perform summarisation of an array of daily snippets into a coherent journal entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-13T14:22:12.512716Z",
     "iopub.status.busy": "2025-03-13T14:22:12.512255Z",
     "iopub.status.idle": "2025-03-13T14:22:12.518555Z",
     "shell.execute_reply": "2025-03-13T14:22:12.517549Z",
     "shell.execute_reply.started": "2025-03-13T14:22:12.512676Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, HTML, display\n",
    "\n",
    "snippets = [\n",
    "    \"The alarm blared, and I instantly wished for more sleep.\",\n",
    "    \"Coffee was a burnt necessity, not a luxury.\",\n",
    "    \"My email inbox was a chaotic, overflowing mess.\",\n",
    "    \"The team meeting was a flurry of ideas, leaving my head spinning.\",\n",
    "    \"Lunch was a rushed sandwich, barely a pause in the day.\",\n",
    "    \"A brief park walk offered a moment of sunshine and bird song.\",\n",
    "    \"Coding became an endless cycle of debugging frustration.\",\n",
    "    \"My kid's call brought a hilarious school story, needing to be remembered.\",\n",
    "    \"Dinner was simple pasta, a comfort against exhaustion.\",\n",
    "    \"A bizarrely fluffy, cloud-like dog crossed my path.\",\n",
    "    \"The grocery run included a movie chat with Sarah.\",\n",
    "    \"Home brought aching feet and the promise of bed.\",\n",
    "    \"My brain buzzed with floating pancake dreams and looming tasks.\",\n",
    "    \"I had to remind myself to call Mom and pay that bill.\",\n",
    "    \"The laundry pile loomed, a testament to my busy life.\",\n",
    "    \"The project deadline pressed down, demanding organization.\",\n",
    "    \"A sudden worry about the locked front door crept in.\",\n",
    "    \"A late-night social media scroll proved a poor choice.\",\n",
    "    \"A strange noise stirred my anxiety, house settling or something else?\",\n",
    "    \"All that remained was the desperate plea for sleep.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T14:27:43.172001Z",
     "iopub.status.busy": "2025-03-13T14:27:43.171681Z",
     "iopub.status.idle": "2025-03-13T14:27:43.175987Z",
     "shell.execute_reply": "2025-03-13T14:27:43.175084Z",
     "shell.execute_reply.started": "2025-03-13T14:27:43.171973Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def display_markdown(checkpoint_name, summary):\n",
    "    lines = summary.splitlines()\n",
    "    quoted_lines = [\"> \" + line for line in lines]\n",
    "    markdown_summary = \"\\n\".join(quoted_lines)\n",
    "    display(Markdown(f\"**{checkpoint_name}:**\\n{markdown_summary}\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5\n",
    "\n",
    "See [documentation](https://huggingface.co/docs/transformers/en/model_doc/t5). T5 models are great for text-to-text tasks with concise prompting. This session explores multiple local T5 models available via HuggingFace transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T13:44:14.652626Z",
     "iopub.status.busy": "2025-03-13T13:44:14.652290Z",
     "iopub.status.idle": "2025-03-13T13:44:14.656241Z",
     "shell.execute_reply": "2025-03-13T13:44:14.655495Z",
     "shell.execute_reply.started": "2025-03-13T13:44:14.652593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "t5_checkpoints = [\n",
    "    \"google-t5/t5-small\",\n",
    "    \"google-t5/t5-base\",\n",
    "    \"google-t5/t5-large\",\n",
    "    # \"google-t5/t5-3b\",\n",
    "    # \"google-t5/t5-11b\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Default Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T13:44:15.435794Z",
     "iopub.status.busy": "2025-03-13T13:44:15.435435Z",
     "iopub.status.idle": "2025-03-13T13:44:19.250234Z",
     "shell.execute_reply": "2025-03-13T13:44:19.249270Z",
     "shell.execute_reply.started": "2025-03-13T13:44:15.435766Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def create_summarizer(checkpoint):\n",
    "    return pipeline(\"summarization\", model=checkpoint)\n",
    "\n",
    "t5_summarizers = [create_summarizer(checkpoint) for checkpoint in t5_checkpoints]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T14:23:42.953199Z",
     "iopub.status.busy": "2025-03-13T14:23:42.952920Z",
     "iopub.status.idle": "2025-03-13T14:23:46.652436Z",
     "shell.execute_reply": "2025-03-13T14:23:46.651753Z",
     "shell.execute_reply.started": "2025-03-13T14:23:42.953174Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**google-t5/t5-small:**\n",
       "> my kid's call brought a hilarious school story, needing to be remembered . my brain buzzed with floating pancake dreams and looming tasks . a sudden worry about the locked front door crept in .\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**google-t5/t5-base:**\n",
       "> cnn's kelly wallace sat down for a few hours to get a good night's sleep . she wished for more sleep after a day of debugging and email . wallace says her brain buzzed with floating pancake dreams and looming tasks .\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**google-t5/t5-large:**\n",
       "> coffee was a burnt necessity, not a luxury, for sarah . her brain buzzed with floating pancake dreams and looming tasks . now she's sharing her tips on how to get more sleep .\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for summarizer in t5_summarizers:\n",
    "    summary = summarizer(\"summarize: \" + \"\\n\".join(snippets))[0][\"summary_text\"]\n",
    "    display_markdown(summarizer.model.name_or_path, summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T14:23:53.555586Z",
     "iopub.status.busy": "2025-03-13T14:23:53.555265Z",
     "iopub.status.idle": "2025-03-13T14:24:24.432926Z",
     "shell.execute_reply": "2025-03-13T14:24:24.431990Z",
     "shell.execute_reply.started": "2025-03-13T14:23:53.555558Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**google-t5/t5-small:**\n",
       "> my kid's call brought a hilarious school story, needing to be remembered. my brain buzzed with floating pancake dreams and looming tasks. a sudden worry about the locked front door crept in. a late-night social media scroll proved a poor choice.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**google-t5/t5-base:**\n",
       "> cnn.com's ireport boot camp challenges ireporters to get more sleep . cnn.com's ireport boot camp challenges ireporters to get more sleep . cnn.com's ireport boot camp challenges ireporters to get more sleep . do you know a hero? nominations are open for 2013 cnn heroes . ireport.com: do you have a story to share? share it with c\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**google-t5/t5-large:**\n",
       "> coffee was a burnt necessity, not a luxury, for sarah . her brain buzzed with floating pancake dreams and looming tasks . sarah's new book, \"sleep, sleep, sleep,\" is out now .\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "for checkpoint in t5_checkpoints:\n",
    "    tokenizer = T5Tokenizer.from_pretrained(checkpoint)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(checkpoint)\n",
    "    \n",
    "    input_ids = tokenizer(\"summarize: \" + \"\\n\".join(snippets), return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(input_ids, min_length=50, max_length=350, num_beams=4, length_penalty=3, do_sample=False)\n",
    "\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    display_markdown(checkpoint, summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T14:24:24.434415Z",
     "iopub.status.busy": "2025-03-13T14:24:24.434114Z",
     "iopub.status.idle": "2025-03-13T14:24:25.833753Z",
     "shell.execute_reply": "2025-03-13T14:24:25.832827Z",
     "shell.execute_reply.started": "2025-03-13T14:24:24.434385Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ee8b71fe6a4d04b685b87f4d6affef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntSlider(value=50, description='Min Length:', max=500, min=50), IntSlider(value=350, descripti…"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-large\")\n",
    "\n",
    "min_length = widgets.IntSlider(value=50, min=50, max=500, description=\"Min Length:\")\n",
    "max_length = widgets.IntSlider(value=350, min=50, max=500, description=\"Max Length:\")\n",
    "num_beams = widgets.IntSlider(value=4, min=1, max=10, description=\"Beams:\")\n",
    "length_penalty = widgets.IntSlider(value=1, min=-3, max=3, description=\"Length Penalty:\")\n",
    "temperature = widgets.FloatSlider(value=1.0, min=0.1, max=2.0, step=0.1, description=\"Temp:\")\n",
    "top_k = widgets.IntSlider(value=50, min=1, max=200, description=\"Top K:\")\n",
    "top_p = widgets.FloatSlider(value=0.95, min=0.1, max=1.0, step=0.05, description=\"Top P:\")\n",
    "\n",
    "button = widgets.Button(description=\"Summarize\")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "spinner = widgets.HTML(\"<i class='fa fa-spinner fa-spin' style='font-size:24px; color:white; padding: 4px'></i>\")\n",
    "spinner.layout.display = 'none'\n",
    "\n",
    "def summarize(b):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        spinner.layout.display = ''\n",
    "        display(spinner)\n",
    "        try:\n",
    "            text_input = \"\\n\".join(snippets)\n",
    "            inputs = tokenizer.encode(\"summarize: \" + text_input, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "            summary_ids = model.generate(\n",
    "                inputs,\n",
    "                min_length=min_length.value,\n",
    "                max_length=max_length.value,\n",
    "                num_beams=num_beams.value,\n",
    "                length_penalty=length_penalty.value,\n",
    "                temperature=temperature.value,\n",
    "                top_k=top_k.value,\n",
    "                top_p=top_p.value,\n",
    "                early_stopping=False\n",
    "            )\n",
    "            summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "            clear_output()\n",
    "            display_markdown(model.name_or_path, summary)\n",
    "        except Exception as e:\n",
    "            clear_output()\n",
    "            print(f\"An error occured: {e}\")\n",
    "        finally:\n",
    "            spinner.layout.display = 'none'\n",
    "\n",
    "button.on_click(summarize)\n",
    "\n",
    "widgets.VBox([min_length, max_length, num_beams, length_penalty, temperature, top_k, top_p, button, output_area])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLAN-T5\n",
    "\n",
    "FLAN-T5 models are particularly well suited for prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T14:25:31.930865Z",
     "iopub.status.busy": "2025-03-13T14:25:31.930584Z",
     "iopub.status.idle": "2025-03-13T14:27:30.859012Z",
     "shell.execute_reply": "2025-03-13T14:27:30.858193Z",
     "shell.execute_reply.started": "2025-03-13T14:25:31.930842Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**google/flan-t5-small:**\n",
       "> In a journal, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippets of daily snippets, snippet\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**google/flan-t5-base:**\n",
       "> The alarm blared, and I instantly wished for more sleep. Coffee was a burnt necessity, not a luxury. My email inbox was a chaotic, overflowing mess. Lunch was a rushed sandwich, barely a pause in the day. Coding became an endless cycle of debugging frustration. My kid's call brought a hilarious school story, needing to be remembered. Dinner was simple pasta, a comfort against exhaustion. A bizarrely fluffy, cloud-like dog crossed my path. The grocery run included a movie chat with Sarah. Home brought aching feet and the promise of bed. My brain buzzed with floating pancake dreams and looming tasks. The project deadline pressed down, demanding organization. A sudden worry about the locked front door crept in. A late-night social media scroll proved a poor choice. A strange noise stirred my anxiety, house settling or something else? All that remains was the desperate plea for sleep.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**google/flan-t5-large:**\n",
       "> The alarm blared, and I instantly wished for more sleep. Coffee was a burnt necessity, not a luxury. My email inbox was a chaotic, overflowing mess. The team meeting was a flurry of ideas, leaving my head spinning. Lunch was a rushed sandwich, barely a pause in the day. A brief park walk offered a moment of sunshine and bird song. Coding became an endless cycle of debugging frustration. My kid's call brought a hilarious school story, needing to be remembered. Dinner was simple pasta, a comfort against exhaustion. A bizarrely fluffy, cloud-like dog crossed my path. The grocery run included a movie chat with Sarah. Home brought aching feet and the promise of bed. My brain buzzed with floating pancake dreams and looming tasks. I had to remind myself to call Mom and pay that bill. The laundry pile loomed, a testament to my busy life. The project deadline pressed down, demanding organization. A sudden worry about the locked front door crept in. A late-night social media scroll proved a poor choice. A strange noise stirred my anxiety, house settling or something else? All that remained was the desperate plea for sleep.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "flan_t5_checkpoints = [\n",
    "    \"google/flan-t5-small\",\n",
    "    \"google/flan-t5-base\",\n",
    "    \"google/flan-t5-large\",\n",
    "]\n",
    "\n",
    "for checkpoint in flan_t5_checkpoints:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        \"Summarise these daily snippets into a coherent journal entry in a reflective and personal tone: \" \n",
    "        + \"\\n\".join(snippets), return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, min_length=50, max_length=350, num_beams=6, length_penalty=3)\n",
    "\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    display_markdown(checkpoint, summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BART fine-tuned on CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T13:45:22.586894Z",
     "iopub.status.busy": "2025-03-12T13:45:22.586626Z",
     "iopub.status.idle": "2025-03-12T13:45:25.045541Z",
     "shell.execute_reply": "2025-03-12T13:45:25.044437Z",
     "shell.execute_reply.started": "2025-03-12T13:45:22.586873Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "print(summarizer(\"\\n\".join(snippets), min_length=50, max_length=350, do_sample=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T13:46:40.525796Z",
     "iopub.status.busy": "2025-03-12T13:46:40.525355Z",
     "iopub.status.idle": "2025-03-12T13:47:07.732613Z",
     "shell.execute_reply": "2025-03-12T13:47:07.731544Z",
     "shell.execute_reply.started": "2025-03-12T13:46:40.525761Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "pegasus_model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")\n",
    "pegasus_tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
    "inputs = tokenizer(\"\\n\".join(snippets), max_length=1024, return_tensors=\"pt\")\n",
    "summary_ids = model.generate(inputs[\"input_ids\"], min_length=50, max_length=350)\n",
    "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T14:27:48.740727Z",
     "iopub.status.busy": "2025-03-13T14:27:48.740379Z",
     "iopub.status.idle": "2025-03-13T14:27:48.978424Z",
     "shell.execute_reply": "2025-03-13T14:27:48.977788Z",
     "shell.execute_reply.started": "2025-03-13T14:27:48.740694Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "from google import genai\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "gemini_apiKey = user_secrets.get_secret(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T14:27:48.979574Z",
     "iopub.status.busy": "2025-03-13T14:27:48.979336Z",
     "iopub.status.idle": "2025-03-13T14:27:50.661966Z",
     "shell.execute_reply": "2025-03-13T14:27:50.661296Z",
     "shell.execute_reply.started": "2025-03-13T14:27:48.979547Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**GEMINI API:**\n",
       "> Today felt like running on a treadmill that kept speeding up. The alarm was a cruel joke, coffee a desperate act, and the workday a blur of emails and brainstorming that left me utterly drained. That brief walk in the park was the only genuine breath of fresh air. The relentless debugging wore me down, and I almost forgot the joy in my kid’s hilarious school story – I need to hold onto those moments.\n",
       "> \n",
       "> Evenings are a fragile truce between comfort and obligation. Simple pasta eased the edge, but then came the grocery run, deadlines looming, and anxieties bubbling up. That fluffy dog, Sarah's impromptu movie review, a fleeting hope in the mess. Now, lying in bed, my mind is still a ping-pong table of pancake dreams, to-dos, and that unsettling house noise. All I want is sleep, a sweet release from the relentless hum of it all. I swear, tomorrow I need to find a way to slow down.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try: \n",
    "    google_client = genai.Client(api_key=gemini_apiKey)\n",
    "    response = google_client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=\"Summarise these daily snippets into a coherent journal entry in a reflective and personal tone in under 250 words: \" + \"\\n\".join(snippets)\n",
    "    )\n",
    "    display_markdown(\"GEMINI API\", response.text)\n",
    "except genai.APIError as e:\n",
    "    print(f\"Gemini API Error: {e}\")\n",
    "    # TODO: use local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
